{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeed8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 15:57:18,625\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /DATA/disk2/checkpoints/system/partialkl/\n",
      "INFO 05-20 15:57:22 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/DATA/disk2/checkpoints/system/partialkl/', speculative_config=None, tokenizer='/DATA/disk2/checkpoints/system/partialkl/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/DATA/disk2/checkpoints/system/partialkl/, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 05-20 15:57:25 model_runner.py:1060] Starting to load model /DATA/disk2/checkpoints/system/partialkl/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.43s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 15:57:28 model_runner.py:1071] Loading model weights took 14.2487 GB\n",
      "INFO 05-20 15:57:29 gpu_executor.py:122] # GPU blocks: 61034, # CPU blocks: 4681\n",
      "INFO 05-20 15:57:29 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 29.80x\n",
      "INFO 05-20 15:57:32 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-20 15:57:32 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-20 15:57:38 model_runner.py:1530] Graph capturing finished in 6 secs.\n",
      "Model initialized.\n",
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:24<00:00,  4.09it/s, est. speed input: 1737.75 toks/s, output: 3677.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndustryOR_fixed\n",
      "0.35\n",
      "Loading data OPTMATH_BENCH.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 166/166 [01:46<00:00,  1.55it/s, est. speed input: 1843.13 toks/s, output: 2628.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTMATH_BENCH\n",
      "0.25301204819277107\n",
      "Loading data MAMO_ComplexLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 211/211 [00:52<00:00,  4.00it/s, est. speed input: 2745.86 toks/s, output: 5083.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAMO_ComplexLP\n",
      "0.6113744075829384\n",
      "Loading data MAMO_EasyLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 652/652 [01:19<00:00,  8.16it/s, est. speed input: 3464.04 toks/s, output: 7003.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAMO_EasyLP\n",
      "0.9003067484662577\n",
      "Loading data NL4OPT.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 245/245 [00:25<00:00,  9.49it/s, est. speed input: 3096.86 toks/s, output: 7121.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL4OPT\n",
      "0.9714285714285714\n",
      "Loading data OptMATH_Bench.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 193/193 [02:01<00:00,  1.59it/s, est. speed input: 1905.13 toks/s, output: 2983.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptMATH_Bench\n",
      "0.3005181347150259\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略所有警告\n",
    "import subprocess\n",
    "from utils import load_jsonl, extract_code_block, extract_obj, change_variable_types\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams        \n",
    "from transformers import AutoTokenizer                                      \n",
    "from langchain.prompts import PromptTemplate\n",
    "from rule_prompt_utils import system_prompt_temp\n",
    "import os\n",
    "\n",
    "def generate_with_model(model, prompt, sampling_params):   \n",
    "    response = model.generate(prompt, sampling_params) \n",
    "    result_text = [g.outputs[0].text for g in response]\n",
    "    return result_text\n",
    "\n",
    "def mp_worker(item):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": zeroshot_prompt_system.format(question=item['en_question']).strip()\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroshot_prompt_user.format(question=item['en_question']).strip()\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def check_result(result_str, item, solver_name='gurobi'):\n",
    "    sub_answer = item['en_answer']\n",
    "    # Convert sub_answer to float or None\n",
    "    sub_answer = None if sub_answer == \"No Best Solution\" or \"-9999\" in str(sub_answer) else float(sub_answer)\n",
    "    \n",
    "    # Extract code snippet\n",
    "    code_snippet = extract_code_block(result_str, solver_name)\n",
    "    if not code_snippet:\n",
    "        return 2\n",
    "    \n",
    "    # Run code snippet\n",
    "    try:\n",
    "        result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Check if execution failed\n",
    "    if result.returncode != 0:\n",
    "        return 3\n",
    "    \n",
    "    # Extract solver result\n",
    "    solver_result = extract_obj(result.stdout)\n",
    "    \n",
    "    # Handle infeasible case or numerical mismatch\n",
    "    if 'nfeasible' in result.stdout or (solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) > 1e-6):\n",
    "        # Try re-running with modified variables\n",
    "        result_str = change_variable_types(result_str)\n",
    "        if result_str:\n",
    "            try:\n",
    "                code_snippet = extract_code_block(result_str, solver_name)\n",
    "                result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "                if result.returncode == 0:\n",
    "                    new_result = extract_obj(result.stdout)\n",
    "                    if 'nfeasible' not in result.stdout:\n",
    "                        if new_result is not None and sub_answer is not None and np.abs(new_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6:\n",
    "                            return 1\n",
    "                        if new_result == sub_answer:\n",
    "                            return 1\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"over_time\")\n",
    "    \n",
    "    # Handle infeasible case after retry\n",
    "    if 'nfeasible' in result.stdout:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Final comparison\n",
    "    if solver_result is not None and sub_answer is not None:\n",
    "        return 1 if np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6 else 0\n",
    "    return 1 if solver_result == sub_answer else 40\n",
    "\n",
    "model_path = '/DATA/disk2/checkpoints/system/partialkl/'\n",
    "tensor_parallel_size = 1\n",
    "# loading models\n",
    "solver_name = 'gurobi'\n",
    "print(\"Loading model\", model_path)\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model initialized.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Load decode strategy\n",
    "topk = 1\n",
    "max_tokens = 8192\n",
    "repetition_penalty = 1.02\n",
    "zeroshot_prompt_system = PromptTemplate.from_template(system_prompt_temp['system'])\n",
    "zeroshot_prompt_user = PromptTemplate.from_template(system_prompt_temp['user'])\n",
    "stop_tokens = [\"</s>\"]\n",
    "sampling_params = SamplingParams(\n",
    "    n=topk,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_tokens=max_tokens,\n",
    "    stop=stop_tokens,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n",
    "\n",
    "# Load data\n",
    "datapath = 'test_data'\n",
    "for filepath in os.listdir(datapath):\n",
    "    print('Loading data', filepath)\n",
    "    loaded_data = load_jsonl(os.path.join(datapath, filepath))\n",
    "    if loaded_data:\n",
    "        pass\n",
    "    test_data = loaded_data\n",
    "    print('Finish Loading')\n",
    "    prompt_list = []\n",
    "    for item in test_data:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    data_name = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    for result_str, item in zip(result_strs, test_data):\n",
    "        snippet_package_cor.append(check_result(result_str, item, solver_name))\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    print(data_name)\n",
    "    print(result[1] / sum(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
