{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeed8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 17:08:48,063\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略所有警告\n",
    "import subprocess\n",
    "from utils import load_jsonl, extract_code_block, extract_obj, change_variable_types\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams        \n",
    "from transformers import AutoTokenizer                                      \n",
    "from langchain.prompts import PromptTemplate\n",
    "from rule_prompt_utils import system_prompt_temp\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887b82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /DATA/disk2/checkpoints/system/partialkl/\n",
      "INFO 05-20 17:08:51 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/DATA/disk2/checkpoints/system/partialkl/', speculative_config=None, tokenizer='/DATA/disk2/checkpoints/system/partialkl/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/DATA/disk2/checkpoints/system/partialkl/, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 05-20 17:08:54 model_runner.py:1060] Starting to load model /DATA/disk2/checkpoints/system/partialkl/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.24s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-20 17:08:57 model_runner.py:1071] Loading model weights took 14.2487 GB\n",
      "INFO 05-20 17:08:58 gpu_executor.py:122] # GPU blocks: 61034, # CPU blocks: 4681\n",
      "INFO 05-20 17:08:58 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 29.80x\n",
      "INFO 05-20 17:09:00 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-20 17:09:00 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-20 17:09:07 model_runner.py:1530] Graph capturing finished in 6 secs.\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# load checkpoints and tokenizer\n",
    "\n",
    "model_path = '/DATA/disk2/checkpoints/system/partialkl/'\n",
    "tensor_parallel_size = 1\n",
    "solver_name = 'gurobi'\n",
    "print(\"Loading model\", model_path)\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model initialized.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6ad7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt template and functions for generation\n",
    "zeroshot_prompt_system = PromptTemplate.from_template(system_prompt_temp['system'])\n",
    "zeroshot_prompt_user = PromptTemplate.from_template(system_prompt_temp['user'])\n",
    "def mp_worker(item):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": zeroshot_prompt_system.format(question=item['en_question']).strip()\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroshot_prompt_user.format(question=item['en_question']).strip()\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def generate_with_model(model, prompt, sampling_params):   \n",
    "    response = model.generate(prompt, sampling_params) \n",
    "    result_text = [g.outputs[0].text for g in response]\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decode strategy\n",
    "topk = 1\n",
    "max_tokens = 8192\n",
    "repetition_penalty = 1.02 # To avoid the occasional occurrence of repeated tokens\n",
    "stop_tokens = [\"</s>\"]\n",
    "\n",
    "# top-p strategy\n",
    "sampling_params = SamplingParams(\n",
    "    n=topk,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_tokens=max_tokens,\n",
    "    stop=stop_tokens,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d32cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pass@1 accuracy\n",
    "def check_result(result_str, item, solver_name='gurobi'):\n",
    "    sub_answer = item['en_answer']\n",
    "    # Convert sub_answer to float or None\n",
    "    sub_answer = None if sub_answer == \"No Best Solution\" or \"-9999\" in str(sub_answer) else float(sub_answer)\n",
    "    \n",
    "    # Extract code snippet\n",
    "    code_snippet = extract_code_block(result_str, solver_name)\n",
    "    if not code_snippet:\n",
    "        return 2\n",
    "    \n",
    "    # Run code snippet\n",
    "    try:\n",
    "        result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Check if execution failed\n",
    "    if result.returncode != 0:\n",
    "        return 3\n",
    "    \n",
    "    # Extract solver result\n",
    "    solver_result = extract_obj(result.stdout)\n",
    "    \n",
    "    # check the first time\n",
    "    if solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) <= 1e-6:\n",
    "        return 1\n",
    "    elif 'nfeasible' in result.stdout: #to handle infeasible case with 'infeasible' or 'Infeasible'\n",
    "        return 1 if sub_answer is None else 0\n",
    "    else:\n",
    "        # Handle infeasible case or numerical mismatch since we ignore the variable types error\n",
    "        result_str = change_variable_types(result_str) # change the type of variables\n",
    "        if result_str:\n",
    "            try:\n",
    "                code_snippet = extract_code_block(result_str, solver_name)\n",
    "                result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "                new_result = extract_obj(result.stdout)\n",
    "                if 'nfeasible' not in result.stdout: #to handle infeasible case with 'infeasible' or 'Infeasible'\n",
    "                    if new_result is not None and sub_answer is not None and np.abs(new_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6:\n",
    "                        return 1\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"over_time\")\n",
    "    \n",
    "    # Handle infeasible case after retry\n",
    "    if 'nfeasible' in result.stdout:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Final comparison\n",
    "    if solver_result is not None and sub_answer is not None:\n",
    "        return 1 if np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6 else 0\n",
    "    return 1 if solver_result == sub_answer else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55e2bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:24<00:00,  4.07it/s, est. speed input: 1729.28 toks/s, output: 3659.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset IndustryOR_fixed.json: 100\n",
      "Numbers of pass@1 cases in dataset IndustryOR_fixed.json: 35\n",
      "pass@1 accuracy for dataset IndustryOR_fixed.json: 35/100 = 0.35\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_166.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 166/166 [01:47<00:00,  1.55it/s, est. speed input: 1835.55 toks/s, output: 2617.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_166.jsonl: 166\n",
      "Numbers of pass@1 cases in dataset OptMATH_Bench_166.jsonl: 42\n",
      "pass@1 accuracy for dataset OptMATH_Bench_166.jsonl: 42/166 = 0.25301204819277107\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_ComplexLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 211/211 [00:52<00:00,  4.00it/s, est. speed input: 2748.09 toks/s, output: 5088.07 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_ComplexLP.json: 211\n",
      "Numbers of pass@1 cases in dataset MAMO_ComplexLP.json: 129\n",
      "pass@1 accuracy for dataset MAMO_ComplexLP.json: 129/211 = 0.6113744075829384\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_EasyLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 652/652 [01:20<00:00,  8.12it/s, est. speed input: 3448.33 toks/s, output: 6971.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_EasyLP.json: 652\n",
      "Numbers of pass@1 cases in dataset MAMO_EasyLP.json: 587\n",
      "pass@1 accuracy for dataset MAMO_EasyLP.json: 587/652 = 0.9003067484662577\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_193.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 193/193 [02:02<00:00,  1.58it/s, est. speed input: 1891.21 toks/s, output: 2946.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_193.jsonl: 193\n",
      "Numbers of pass@1 cases in dataset OptMATH_Bench_193.jsonl: 54\n",
      "pass@1 accuracy for dataset OptMATH_Bench_193.jsonl: 54/193 = 0.27979274611398963\n",
      "-------------------------------------------------------------------\n",
      "Loading data NL4OPT.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 245/245 [00:25<00:00,  9.43it/s, est. speed input: 3077.40 toks/s, output: 7082.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset NL4OPT.jsonl: 245\n",
      "Numbers of pass@1 cases in dataset NL4OPT.jsonl: 238\n",
      "pass@1 accuracy for dataset NL4OPT.jsonl: 238/245 = 0.9714285714285714\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the checkpoint\n",
    "datapath = 'test_data'\n",
    "for filepath in os.listdir(datapath):\n",
    "    \n",
    "    # loading data\n",
    "    print('Loading data', filepath)\n",
    "    test_data = load_jsonl(os.path.join(datapath, filepath))\n",
    "    print('Finish Loading')\n",
    "    \n",
    "    # generation \n",
    "    prompt_list = []\n",
    "    for item in test_data:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    \n",
    "    # check the pass@1 accuracy\n",
    "    for result_str, item in zip(result_strs, test_data):\n",
    "        snippet_package_cor.append(check_result(result_str, item, solver_name))\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\n",
    "    print(f'Numbers of pass@1 cases in dataset {filepath}: {result[1]}')\n",
    "    print(f'pass@1 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\n",
    "    print('-------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
