{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeed8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 17:34:33 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 17:34:34,690\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import subprocess\n",
    "from utils import load_jsonl, extract_code_block, extract_obj, change_variable_types\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams        \n",
    "from transformers import AutoTokenizer                                      \n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887b82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /DATA/disk1/cml/sft_copt/Qwen3_8B_sft\n",
      "INFO 06-18 14:45:25 [config.py:823] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-18 14:45:25 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 06-18 14:45:26 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 06-18 14:45:27 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 06-18 14:45:30 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-18 14:45:32 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-18 14:45:32 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/DATA/disk1/cml/sft_copt/Qwen3_8B_sft', speculative_config=None, tokenizer='/DATA/disk1/cml/sft_copt/Qwen3_8B_sft', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/DATA/disk1/cml/sft_copt/Qwen3_8B_sft, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-18 14:45:33 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f7deb0613a0>\n",
      "INFO 06-18 14:45:33 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-18 14:45:33 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-18 14:45:33 [gpu_model_runner.py:1595] Starting to load model /DATA/disk1/cml/sft_copt/Qwen3_8B_sft...\n",
      "INFO 06-18 14:45:33 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 06-18 14:45:33 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.67it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  2.26it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:01,  1.90it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.65it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.65it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:02<00:00,  1.73it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 14:45:36 [default_loader.py:272] Loading weights took 3.00 seconds\n",
      "INFO 06-18 14:45:37 [gpu_model_runner.py:1624] Model loading took 15.2683 GiB and 3.140523 seconds\n",
      "INFO 06-18 14:45:43 [backends.py:462] Using cache directory: /DATA/disk1/cml/config/cache/vllm/torch_compile_cache/6bfa125916/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-18 14:45:43 [backends.py:472] Dynamo bytecode transform time: 6.08 s\n",
      "INFO 06-18 14:45:48 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 4.300 s\n",
      "INFO 06-18 14:45:48 [monitor.py:34] torch.compile takes 6.08 s in total\n",
      "INFO 06-18 14:45:49 [gpu_worker.py:227] Available KV cache memory: 50.29 GiB\n",
      "INFO 06-18 14:45:50 [kv_cache_utils.py:715] GPU KV cache size: 366,192 tokens\n",
      "INFO 06-18 14:45:50 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 8.94x\n",
      "INFO 06-18 14:46:08 [gpu_model_runner.py:2048] Graph capturing finished in 18 secs, took 0.74 GiB\n",
      "INFO 06-18 14:46:08 [core.py:171] init engine (profile, create kv cache, warmup model) took 30.83 seconds\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# load checkpoints and tokenizer\n",
    "\n",
    "model_path = '/DATA/disk1/cml/sft_copt/Qwen3_8B_sft'\n",
    "tensor_parallel_size = 1\n",
    "solver_name = 'gurobi'\n",
    "print(\"Loading model\", model_path)\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model initialized.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ada6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /DATA/disk1/cml/sft_copt/Qwen3_8B_sft\n",
      "INFO 06-18 17:34:46 [config.py:823] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-18 17:34:46 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 06-18 17:34:48 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 06-18 17:34:49 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 06-18 17:34:52 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-18 17:34:55 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-18 17:34:55 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/DATA/disk1/cml/sft_copt/Qwen3_8B_sft', speculative_config=None, tokenizer='/DATA/disk1/cml/sft_copt/Qwen3_8B_sft', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/DATA/disk1/cml/sft_copt/Qwen3_8B_sft, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-18 17:34:55 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f04222e0350>\n",
      "INFO 06-18 17:34:55 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-18 17:34:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-18 17:34:55 [gpu_model_runner.py:1595] Starting to load model /DATA/disk1/cml/sft_copt/Qwen3_8B_sft...\n",
      "INFO 06-18 17:34:55 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 06-18 17:34:56 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:02,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:01,  2.04it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:01,  1.69it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:02<00:00,  1.53it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:03<00:00,  1.61it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-18 17:34:59 [default_loader.py:272] Loading weights took 3.16 seconds\n",
      "INFO 06-18 17:34:59 [gpu_model_runner.py:1624] Model loading took 15.2683 GiB and 3.297829 seconds\n",
      "INFO 06-18 17:35:06 [backends.py:462] Using cache directory: /DATA/disk1/cml/config/cache/vllm/torch_compile_cache/6bfa125916/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-18 17:35:06 [backends.py:472] Dynamo bytecode transform time: 6.13 s\n",
      "INFO 06-18 17:35:10 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 4.283 s\n",
      "INFO 06-18 17:35:11 [monitor.py:34] torch.compile takes 6.13 s in total\n",
      "INFO 06-18 17:35:12 [gpu_worker.py:227] Available KV cache memory: 50.29 GiB\n",
      "INFO 06-18 17:35:12 [kv_cache_utils.py:715] GPU KV cache size: 366,192 tokens\n",
      "INFO 06-18 17:35:12 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 8.94x\n",
      "INFO 06-18 17:35:31 [gpu_model_runner.py:2048] Graph capturing finished in 19 secs, took 0.74 GiB\n",
      "INFO 06-18 17:35:31 [core.py:171] init engine (profile, create kv cache, warmup model) took 32.21 seconds\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# load checkpoints and tokenizer\n",
    "\n",
    "model_path = '/DATA/disk1/cml/sft_copt/Qwen3_8B_sft'\n",
    "tensor_parallel_size = 1\n",
    "solver_name = 'copt'\n",
    "print(\"Loading model\", model_path)\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model initialized.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ad7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt template and functions for Gurobi generation\n",
    "from rule_prompt_utils import system_prompt_temp\n",
    "zeroshot_prompt_system = PromptTemplate.from_template(system_prompt_temp['system'])\n",
    "zeroshot_prompt_user = PromptTemplate.from_template(system_prompt_temp['user'])\n",
    "def mp_worker(item):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": zeroshot_prompt_system.format(question=item['en_question']).strip()\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroshot_prompt_user.format(question=item['en_question']).strip()\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def generate_with_model(model, prompt, sampling_params):   \n",
    "    response = model.generate(prompt, sampling_params) \n",
    "    result_text = [g.outputs[0].text for g in response]\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4867b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt template and functions for Copt generation\n",
    "from rule_prompt_utils_copt import system_prompt_temp\n",
    "zeroshot_prompt_system = PromptTemplate.from_template(system_prompt_temp['system'])\n",
    "zeroshot_prompt_user = PromptTemplate.from_template(system_prompt_temp['user'])\n",
    "def mp_worker(item):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": zeroshot_prompt_system.format(question=item['en_question']).strip()\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroshot_prompt_user.format(question=item['en_question']).strip()\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def generate_with_model(model, prompt, sampling_params):   \n",
    "    response = model.generate(prompt, sampling_params) \n",
    "    result_text = [g.outputs[0].text for g in response]\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9583ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decode strategy\n",
    "topk = 1\n",
    "max_tokens = 8192\n",
    "repetition_penalty = 1.02 # To avoid the occasional occurrence of repeated tokens\n",
    "stop_tokens = [\"</s>\"]\n",
    "\n",
    "# top-p strategy\n",
    "sampling_params = SamplingParams(\n",
    "    n=topk,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_tokens=max_tokens,\n",
    "    stop=stop_tokens,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d32cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pass@1 accuracy for Gurobi\n",
    "def check_result(result_str, item, solver_name='gurobi'):\n",
    "    sub_answer = item['en_answer']\n",
    "    # Convert sub_answer to float or None\n",
    "    sub_answer = None if sub_answer == \"No Best Solution\" or \"-9999\" in str(sub_answer) else float(sub_answer)\n",
    "    \n",
    "    # Extract code snippet\n",
    "    code_snippet = extract_code_block(result_str, solver_name)\n",
    "    #print(code_snippet)\n",
    "    if not code_snippet:\n",
    "        return 2\n",
    "    \n",
    "    # Run code snippet\n",
    "    try:\n",
    "        result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Check if execution failed\n",
    "    if result.returncode != 0:\n",
    "        return 3\n",
    "    \n",
    "    # Extract solver result\n",
    "    solver_result = extract_obj(result.stdout)\n",
    "    \n",
    "    # check the first time\n",
    "    if solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) <= 1e-6:\n",
    "        return 1\n",
    "    # Handle infeasible case or numerical mismatch since we ignore the variable types error\n",
    "    if 'nfeasible' in result.stdout or (solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) > 1e-6):\n",
    "        # Try re-running with modified variables: we ignore the variable types error\n",
    "        result_str = change_variable_types(result_str) # change the type of variables\n",
    "        if result_str:\n",
    "            try:\n",
    "                code_snippet = extract_code_block(result_str, solver_name)\n",
    "                result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "                if result.returncode == 0:\n",
    "                    new_result = extract_obj(result.stdout)\n",
    "                    if 'nfeasible' not in result.stdout: # infeasible and Infeasible\n",
    "                        if new_result is not None and sub_answer is not None and np.abs(new_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6:\n",
    "                            return 1\n",
    "                        if new_result == sub_answer:\n",
    "                            return 1\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"over_time\")\n",
    "    \n",
    "    # Handle infeasible case after retry\n",
    "    if 'nfeasible' in result.stdout:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Final comparison\n",
    "    if solver_result is not None and sub_answer is not None:\n",
    "        return 1 if np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6 else 0\n",
    "    return 1 if solver_result == sub_answer else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb373c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pass@1 accuracy for Copt\n",
    "def check_result(result_str, item, solver_name='copt'):\n",
    "    sub_answer = item['en_answer']\n",
    "    # Convert sub_answer to float or None\n",
    "    sub_answer = None if sub_answer == \"No Best Solution\" or \"-9999\" in str(sub_answer) else float(sub_answer)\n",
    "    \n",
    "    # Extract code snippet\n",
    "    code_snippet = extract_code_block(result_str, solver_name)\n",
    "    with open('ouput_code.txt','a',encoding='utf-8') as f:\n",
    "        f.write(f'{code_snippet}\\n')\n",
    "    #print(code_snippet)\n",
    "    if not code_snippet:\n",
    "        return 2\n",
    "    \n",
    "    # Run code snippet\n",
    "    try:\n",
    "        result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Check if execution failed\n",
    "    if result.returncode != 0:\n",
    "        return 3\n",
    "    \n",
    "    # Extract solver result\n",
    "    solver_result = extract_obj(result.stdout)\n",
    "    \n",
    "    # check the first time\n",
    "    if solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) <= 1e-6:\n",
    "        return 1\n",
    "    # Handle infeasible case or numerical mismatch since we ignore the variable types error\n",
    "    if 'nfeasible' in result.stdout or (solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) > 1e-6):\n",
    "        # Try re-running with modified variables: we ignore the variable types error\n",
    "        result_str = change_variable_types(result_str) # change the type of variables\n",
    "        if result_str:\n",
    "            try:\n",
    "                code_snippet = extract_code_block(result_str, solver_name)\n",
    "                result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "                if result.returncode == 0:\n",
    "                    new_result = extract_obj(result.stdout)\n",
    "                    if 'nfeasible' not in result.stdout: # infeasible and Infeasible\n",
    "                        if new_result is not None and sub_answer is not None and np.abs(new_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6:\n",
    "                            return 1\n",
    "                        if new_result == sub_answer:\n",
    "                            return 1\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"over_time\")\n",
    "    \n",
    "    # Handle infeasible case after retry\n",
    "    if 'nfeasible' in result.stdout:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Final comparison\n",
    "    if solver_result is not None and sub_answer is not None:\n",
    "        return 1 if np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6 else 0\n",
    "    return 1 if solver_result == sub_answer else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e2bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data NL4OPT.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 0/245 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 245/245 [00:00<00:00, 1363.79it/s]\n",
      "Processed prompts: 100%|██████████| 245/245 [03:11<00:00,  1.28it/s, est. speed input: 413.52 toks/s, output: 4260.03 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset NL4OPT.jsonl: 245\n",
      "Numbers of pass@1 cases in dataset NL4OPT.jsonl: 207\n",
      "pass@1 accuracy for dataset NL4OPT.jsonl: 207/245 = 0.8448979591836735\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_EasyLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 652/652 [00:00<00:00, 1210.94it/s]\n",
      "Processed prompts: 100%|██████████| 652/652 [09:47<00:00,  1.11it/s, est. speed input: 467.48 toks/s, output: 3993.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_EasyLP.json: 652\n",
      "Numbers of pass@1 cases in dataset MAMO_EasyLP.json: 477\n",
      "pass@1 accuracy for dataset MAMO_EasyLP.json: 477/652 = 0.7315950920245399\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_ComplexLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 211/211 [00:00<00:00, 749.10it/s]\n",
      "Processed prompts: 100%|██████████| 211/211 [05:42<00:00,  1.62s/it, est. speed input: 422.09 toks/s, output: 2997.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_ComplexLP.json: 211\n",
      "Numbers of pass@1 cases in dataset MAMO_ComplexLP.json: 77\n",
      "pass@1 accuracy for dataset MAMO_ComplexLP.json: 77/211 = 0.36492890995260663\n",
      "-------------------------------------------------------------------\n",
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 100/100 [00:00<00:00, 988.46it/s]\n",
      "Processed prompts: 100%|██████████| 100/100 [03:07<00:00,  1.87s/it, est. speed input: 225.13 toks/s, output: 3092.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset IndustryOR_fixed.json: 100\n",
      "Numbers of pass@1 cases in dataset IndustryOR_fixed.json: 34\n",
      "pass@1 accuracy for dataset IndustryOR_fixed.json: 34/100 = 0.34\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_193.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 193/193 [00:00<00:00, 522.06it/s]\n",
      "Processed prompts: 100%|██████████| 193/193 [07:30<00:00,  2.33s/it, est. speed input: 511.64 toks/s, output: 2610.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_193.jsonl: 193\n",
      "Numbers of pass@1 cases in dataset OptMATH_Bench_193.jsonl: 23\n",
      "pass@1 accuracy for dataset OptMATH_Bench_193.jsonl: 23/193 = 0.11917098445595854\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_166.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 166/166 [00:00<00:00, 515.84it/s]\n",
      "Processed prompts: 100%|██████████| 166/166 [06:45<00:00,  2.44s/it, est. speed input: 484.73 toks/s, output: 2603.75 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_166.jsonl: 166\n",
      "Numbers of pass@1 cases in dataset OptMATH_Bench_166.jsonl: 15\n",
      "pass@1 accuracy for dataset OptMATH_Bench_166.jsonl: 15/166 = 0.09036144578313253\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptiBench.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 605/605 [00:00<00:00, 1403.16it/s]\n",
      "Processed prompts: 100%|██████████| 605/605 [09:28<00:00,  1.06it/s, est. speed input: 384.29 toks/s, output: 3974.45 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptiBench.jsonl: 605\n",
      "Numbers of pass@1 cases in dataset OptiBench.jsonl: 365\n",
      "pass@1 accuracy for dataset OptiBench.jsonl: 365/605 = 0.6033057851239669\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if you want to check pass@1 accuracy, please run this cell\n",
    "# Test the checkpoint\n",
    "datapath = 'test_data'\n",
    "testdataset = ['NL4OPT.jsonl', 'MAMO_EasyLP.json', 'MAMO_ComplexLP.json', 'IndustryOR_fixed.json', 'OptMATH_Bench_193.jsonl', 'OptMATH_Bench_166.jsonl','OptiBench.jsonl']\n",
    "for filepath in testdataset:\n",
    "    \n",
    "    # loading data\n",
    "    print('Loading data', filepath)\n",
    "    test_data = load_jsonl(os.path.join(datapath, filepath))\n",
    "    print('Finish Loading')\n",
    "    \n",
    "    # generation \n",
    "    \n",
    "    prompt_list = []\n",
    "    for item in test_data:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    # check the pass@1 accuracy\n",
    "    \n",
    "    for result_str, item in zip(result_strs, test_data):\n",
    "        snippet_package_cor.append(check_result(result_str, item, solver_name))\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\n",
    "    print(f'Numbers of pass@1 cases in dataset {filepath}: {result[1]}')\n",
    "    print(f'pass@1 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc02345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 20/20 [00:00<00:00, 696.53it/s]\n",
      "Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [01:27<00:00,  4.38s/it, est. speed input: 100.62 toks/s, output: 1197.47 toks/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'pattern' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m   \u001b[38;5;66;03m# check the pass@1 accuracy\u001b[39;00m\n\u001b[32m     22\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m result_str, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result_strs, test_data[:\u001b[32m20\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m       snippet_package_cor.append(\u001b[43mcheck_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     24\u001b[39m   result = np.bincount(snippet_package_cor)\n\u001b[32m     25\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m  print(f'Numbers of pass@1 cases in dataset {filepath}: {result[1]}')\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m  print(f'pass@1 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m  print('-------------------------------------------------------------------')\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcheck_result\u001b[39m\u001b[34m(result_str, item, solver_name)\u001b[39m\n\u001b[32m      5\u001b[39m sub_answer = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m sub_answer == \u001b[33m\"\u001b[39m\u001b[33mNo Best Solution\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m-9999\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(sub_answer) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(sub_answer)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract code snippet\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m code_snippet = \u001b[43mextract_code_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mouput_code.txt\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m,encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     10\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_snippet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DATA/disk1/cml/SIRL/utils.py:86\u001b[39m, in \u001b[36mextract_code_block\u001b[39m\u001b[34m(llm_output, solver_name)\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[32m     85\u001b[39m             code = match.group(\u001b[32m1\u001b[39m).strip()\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     code = \u001b[43minsert_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m code\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# the python code block is not in <python> </python> format\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# try to extract it using ```python ``` format\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/DATA/disk1/cml/SIRL/utils.py:68\u001b[39m, in \u001b[36minsert_print\u001b[39m\u001b[34m(code, solver_name)\u001b[39m\n\u001b[32m     61\u001b[39m         status_check = (\n\u001b[32m     62\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mif \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.status == GRB.OPTIMAL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m    print(f\u001b[39m\u001b[33m'\u001b[39m\u001b[33mJust print the best solution: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.ObjVal\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33melse:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m    print(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNo optimal solution found, status:\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.status)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         )\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# use re to match the pattern and keep the indent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     code = re.sub(\u001b[43mpattern\u001b[49m, \u001b[33mrf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, code, flags=re.M)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'pattern' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# if you want to check pass@1 accuracy, please run this cell\n",
    "# Test the checkpoint\n",
    "datapath = 'test_data'\n",
    "testdataset = ['NL4OPT.jsonl', 'MAMO_EasyLP.json', 'MAMO_ComplexLP.json', 'IndustryOR_fixed.json', 'OptMATH_Bench_193.jsonl', 'OptMATH_Bench_166.jsonl','OptiBench.jsonl']\n",
    "for filepath in testdataset[3:4]:\n",
    "    \n",
    "    # loading data\n",
    "    print('Loading data', filepath)\n",
    "    test_data = load_jsonl(os.path.join(datapath, filepath))\n",
    "    print('Finish Loading')\n",
    "    \n",
    "    # generation \n",
    "    \n",
    "    prompt_list = []\n",
    "    for item in test_data[:20]:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    # check the pass@1 accuracy\n",
    "    \n",
    "    for result_str, item in zip(result_strs, test_data[:20]):\n",
    "        snippet_package_cor.append(check_result(result_str, item, solver_name))\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    \"\"\"print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\n",
    "    print(f'Numbers of pass@1 cases in dataset {filepath}: {result[1]}')\n",
    "    print(f'pass@1 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c8df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data NL4OPT.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1960/1960 [03:06<00:00, 10.50it/s, est. speed input: 3428.87 toks/s, output: 7892.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset NL4OPT.jsonl: 245\n",
      "Numbers of pass@8 cases in dataset NL4OPT.jsonl: 238\n",
      "pass@8 accuracy for dataset NL4OPT.jsonl: 238/245 = 0.9714285714285714\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_EasyLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5216/5216 [09:39<00:00,  9.01it/s, est. speed input: 3822.21 toks/s, output: 7719.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_EasyLP.json: 652\n",
      "Numbers of pass@8 cases in dataset MAMO_EasyLP.json: 589\n",
      "pass@8 accuracy for dataset MAMO_EasyLP.json: 589/652 = 0.9033742331288344\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_ComplexLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1688/1688 [05:11<00:00,  5.42it/s, est. speed input: 3726.87 toks/s, output: 6914.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_ComplexLP.json: 211\n",
      "Numbers of pass@8 cases in dataset MAMO_ComplexLP.json: 133\n",
      "pass@8 accuracy for dataset MAMO_ComplexLP.json: 133/211 = 0.6303317535545023\n",
      "-------------------------------------------------------------------\n",
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [02:34<00:00,  5.17it/s, est. speed input: 2199.02 toks/s, output: 4772.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset IndustryOR_fixed.json: 100\n",
      "Numbers of pass@8 cases in dataset IndustryOR_fixed.json: 39\n",
      "pass@8 accuracy for dataset IndustryOR_fixed.json: 39/100 = 0.39\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_193.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1544/1544 [09:32<00:00,  2.70it/s, est. speed input: 3230.20 toks/s, output: 5030.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_193.jsonl: 193\n",
      "Numbers of pass@8 cases in dataset OptMATH_Bench_193.jsonl: 68\n",
      "pass@8 accuracy for dataset OptMATH_Bench_193.jsonl: 68/193 = 0.35233160621761656\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_166.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1328/1328 [07:53<00:00,  2.81it/s, est. speed input: 3330.11 toks/s, output: 4967.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_166.jsonl: 166\n",
      "Numbers of pass@8 cases in dataset OptMATH_Bench_166.jsonl: 46\n",
      "pass@8 accuracy for dataset OptMATH_Bench_166.jsonl: 46/166 = 0.27710843373493976\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptiBench.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4840/4840 [08:32<00:00,  9.44it/s, est. speed input: 3435.35 toks/s, output: 7643.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptiBench.jsonl: 605\n",
      "Numbers of pass@8 cases in dataset OptiBench.jsonl: 379\n",
      "pass@8 accuracy for dataset OptiBench.jsonl: 379/605 = 0.6264462809917355\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if you want to check pass@8 accuracy, please run this cell\n",
    "# Test the checkpoint\n",
    "datapath = 'test_data'\n",
    "testdataset = ['NL4OPT.jsonl', 'MAMO_EasyLP.json', 'MAMO_ComplexLP.json', 'IndustryOR_fixed.json', 'OptMATH_Bench_193.jsonl', 'OptMATH_Bench_166.jsonl','OptiBench.jsonl']\n",
    "for filepath in testdataset:\n",
    "    \n",
    "    # loading data\n",
    "    print('Loading data', filepath)\n",
    "    test_data = [i for i in load_jsonl(os.path.join(datapath, filepath)) for _ in range(8)]\n",
    "    print('Finish Loading')\n",
    "    \n",
    "    # generation \n",
    "    \n",
    "    prompt_list = []\n",
    "    for item in test_data:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    snippet_package_tmp=[]\n",
    "    # check the pass@8 accuracy\n",
    "    \n",
    "    result_chunks = [result_strs[i:i + 8] for i in range(0, len(result_strs), 8)]\n",
    "    test_data_chunks = [test_data[i:i + 8] for i in range(0, len(test_data), 8)]\n",
    "    for result_chunk, items in zip(result_chunks,test_data_chunks):\n",
    "        for chunk, item in zip(result_chunk, items):\n",
    "            snippet_package_tmp.append(check_result(chunk, item, solver_name))\n",
    "        if 1 in snippet_package_tmp:\n",
    "            snippet_package_cor.append(1)\n",
    "        else:\n",
    "            snippet_package_cor.append(0)\n",
    "        snippet_package_tmp.clear()\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\n",
    "    print(f'Numbers of pass@8 cases in dataset {filepath}: {result[1]}')\n",
    "    print(f'pass@8 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
