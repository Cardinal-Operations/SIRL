{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeed8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 16:17:37,357\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import subprocess\n",
    "from utils import load_jsonl, extract_code_block, extract_obj, change_variable_types\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams        \n",
    "from transformers import AutoTokenizer                                      \n",
    "from langchain.prompts import PromptTemplate\n",
    "from rule_prompt_utils import system_prompt_temp\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887b82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /DATA/disk1/cml/config/MScache/models/oneday88/SIRL-7B\n",
      "INFO 06-01 16:17:45 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/DATA/disk1/cml/config/MScache/models/oneday88/SIRL-7B', speculative_config=None, tokenizer='/DATA/disk1/cml/config/MScache/models/oneday88/SIRL-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/DATA/disk1/cml/config/MScache/models/oneday88/SIRL-7B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 06-01 16:17:47 model_runner.py:1060] Starting to load model /DATA/disk1/cml/config/MScache/models/oneday88/SIRL-7B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-01 16:17:49 model_runner.py:1071] Loading model weights took 14.2487 GB\n",
      "INFO 06-01 16:17:50 gpu_executor.py:122] # GPU blocks: 61034, # CPU blocks: 4681\n",
      "INFO 06-01 16:17:50 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 29.80x\n",
      "INFO 06-01 16:17:52 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-01 16:17:52 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-01 16:18:00 model_runner.py:1530] Graph capturing finished in 8 secs.\n",
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "# load checkpoints and tokenizer\n",
    "\n",
    "model_path = '/DATA/disk1/cml/config/MScache/models/oneday88/SIRL-7B'\n",
    "tensor_parallel_size = 1\n",
    "solver_name = 'gurobi'\n",
    "print(\"Loading model\", model_path)\n",
    "model = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model initialized.\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6ad7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt template and functions for generation\n",
    "zeroshot_prompt_system = PromptTemplate.from_template(system_prompt_temp['system'])\n",
    "zeroshot_prompt_user = PromptTemplate.from_template(system_prompt_temp['user'])\n",
    "def mp_worker(item):\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": zeroshot_prompt_system.format(question=item['en_question']).strip()\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": zeroshot_prompt_user.format(question=item['en_question']).strip()\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def generate_with_model(model, prompt, sampling_params):   \n",
    "    response = model.generate(prompt, sampling_params) \n",
    "    result_text = [g.outputs[0].text for g in response]\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9583ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decode strategy\n",
    "topk = 1\n",
    "max_tokens = 8192\n",
    "repetition_penalty = 1.02 # To avoid the occasional occurrence of repeated tokens\n",
    "stop_tokens = [\"</s>\"]\n",
    "\n",
    "# top-p strategy\n",
    "sampling_params = SamplingParams(\n",
    "    n=topk,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_tokens=max_tokens,\n",
    "    stop=stop_tokens,\n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d32cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pass@1 accuracy\n",
    "def check_result(result_str, item, solver_name='gurobi'):\n",
    "    sub_answer = item['en_answer']\n",
    "    # Convert sub_answer to float or None\n",
    "    sub_answer = None if sub_answer == \"No Best Solution\" or \"-9999\" in str(sub_answer) else float(sub_answer)\n",
    "    \n",
    "    # Extract code snippet\n",
    "    code_snippet = extract_code_block(result_str, solver_name)\n",
    "    if not code_snippet:\n",
    "        return 2\n",
    "    \n",
    "    # Run code snippet\n",
    "    try:\n",
    "        result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Check if execution failed\n",
    "    if result.returncode != 0:\n",
    "        return 3\n",
    "    \n",
    "    # Extract solver result\n",
    "    solver_result = extract_obj(result.stdout)\n",
    "    \n",
    "    # check the first time\n",
    "    if solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) <= 1e-6:\n",
    "        return 1\n",
    "    # Handle infeasible case or numerical mismatch since we ignore the variable types error\n",
    "    if 'nfeasible' in result.stdout or (solver_result is not None and sub_answer is not None and np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) > 1e-6):\n",
    "        # Try re-running with modified variables: we ignore the variable types error\n",
    "        result_str = change_variable_types(result_str) # change the type of variables\n",
    "        if result_str:\n",
    "            try:\n",
    "                code_snippet = extract_code_block(result_str, solver_name)\n",
    "                result = subprocess.run(['python3', '-c', code_snippet], capture_output=True, text=True, timeout=100)\n",
    "                if result.returncode == 0:\n",
    "                    new_result = extract_obj(result.stdout)\n",
    "                    if 'nfeasible' not in result.stdout: # infeasible and Infeasible\n",
    "                        if new_result is not None and sub_answer is not None and np.abs(new_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6:\n",
    "                            return 1\n",
    "                        if new_result == sub_answer:\n",
    "                            return 1\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"over_time\")\n",
    "    \n",
    "    # Handle infeasible case after retry\n",
    "    if 'nfeasible' in result.stdout:\n",
    "        return 1 if sub_answer is None else 0\n",
    "    \n",
    "    # Final comparison\n",
    "    if solver_result is not None and sub_answer is not None:\n",
    "        return 1 if np.abs(solver_result - sub_answer) / (np.abs(sub_answer) + 1) < 1e-6 else 0\n",
    "    return 1 if solver_result == sub_answer else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55e2bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data NL4OPT.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 245/245 [00:24<00:00,  9.80it/s, est. speed input: 3199.33 toks/s, output: 7358.92 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset NL4OPT.jsonl: 245\n",
      "Numbers of pass@1 cases in dataset NL4OPT.jsonl: 235\n",
      "pass@1 accuracy for dataset NL4OPT.jsonl: 235/245 = 0.9591836734693877\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_EasyLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 652/652 [01:18<00:00,  8.28it/s, est. speed input: 3514.86 toks/s, output: 7094.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_EasyLP.json: 652\n",
      "Numbers of pass@1 cases in dataset MAMO_EasyLP.json: 587\n",
      "pass@1 accuracy for dataset MAMO_EasyLP.json: 587/652 = 0.9003067484662577\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_ComplexLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 211/211 [00:53<00:00,  3.96it/s, est. speed input: 2720.62 toks/s, output: 5038.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_ComplexLP.json: 211\n",
      "Numbers of pass@1 cases in dataset MAMO_ComplexLP.json: 131\n",
      "pass@1 accuracy for dataset MAMO_ComplexLP.json: 131/211 = 0.6208530805687204\n",
      "-------------------------------------------------------------------\n",
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:23<00:00,  4.17it/s, est. speed input: 1774.68 toks/s, output: 3796.97 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset IndustryOR_fixed.json: 100\n",
      "Numbers of pass@1 cases in dataset IndustryOR_fixed.json: 33\n",
      "pass@1 accuracy for dataset IndustryOR_fixed.json: 33/100 = 0.33\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_193.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 193/193 [01:57<00:00,  1.64it/s, est. speed input: 1966.92 toks/s, output: 2941.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_193.jsonl: 193\n",
      "Numbers of pass@1 cases in dataset OptMATH_Bench_193.jsonl: 56\n",
      "pass@1 accuracy for dataset OptMATH_Bench_193.jsonl: 56/193 = 0.29015544041450775\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_166.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 166/166 [01:49<00:00,  1.52it/s, est. speed input: 1804.35 toks/s, output: 2685.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_166.jsonl: 166\n",
      "Numbers of pass@1 cases in dataset OptMATH_Bench_166.jsonl: 42\n",
      "pass@1 accuracy for dataset OptMATH_Bench_166.jsonl: 42/166 = 0.25301204819277107\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptiBench.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 605/605 [01:09<00:00,  8.68it/s, est. speed input: 3161.29 toks/s, output: 7034.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptiBench.jsonl: 605\n",
      "Numbers of pass@1 cases in dataset OptiBench.jsonl: 351\n",
      "pass@1 accuracy for dataset OptiBench.jsonl: 351/605 = 0.5801652892561984\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if you want to check pass@1 accuracy, please run this cell\n",
    "# Test the checkpoint\n",
    "datapath = 'test_data'\n",
    "testdataset = ['NL4OPT.jsonl', 'MAMO_EasyLP.json', 'MAMO_ComplexLP.json', 'IndustryOR_fixed.json', 'OptMATH_Bench_193.jsonl', 'OptMATH_Bench_166.jsonl','OptiBench.jsonl']\n",
    "for filepath in testdataset:\n",
    "    \n",
    "    # loading data\n",
    "    print('Loading data', filepath)\n",
    "    test_data = load_jsonl(os.path.join(datapath, filepath))\n",
    "    print('Finish Loading')\n",
    "    \n",
    "    # generation \n",
    "    \n",
    "    prompt_list = []\n",
    "    for item in test_data:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    # check the pass@1 accuracy\n",
    "    \n",
    "    for result_str, item in zip(result_strs, test_data):\n",
    "        snippet_package_cor.append(check_result(result_str, item, solver_name))\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\n",
    "    print(f'Numbers of pass@1 cases in dataset {filepath}: {result[1]}')\n",
    "    print(f'pass@1 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c8df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data NL4OPT.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1960/1960 [03:06<00:00, 10.50it/s, est. speed input: 3428.87 toks/s, output: 7892.34 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset NL4OPT.jsonl: 245\n",
      "Numbers of pass@8 cases in dataset NL4OPT.jsonl: 238\n",
      "pass@8 accuracy for dataset NL4OPT.jsonl: 238/245 = 0.9714285714285714\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_EasyLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5216/5216 [09:39<00:00,  9.01it/s, est. speed input: 3822.21 toks/s, output: 7719.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_EasyLP.json: 652\n",
      "Numbers of pass@8 cases in dataset MAMO_EasyLP.json: 589\n",
      "pass@8 accuracy for dataset MAMO_EasyLP.json: 589/652 = 0.9033742331288344\n",
      "-------------------------------------------------------------------\n",
      "Loading data MAMO_ComplexLP.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1688/1688 [05:11<00:00,  5.42it/s, est. speed input: 3726.87 toks/s, output: 6914.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset MAMO_ComplexLP.json: 211\n",
      "Numbers of pass@8 cases in dataset MAMO_ComplexLP.json: 133\n",
      "pass@8 accuracy for dataset MAMO_ComplexLP.json: 133/211 = 0.6303317535545023\n",
      "-------------------------------------------------------------------\n",
      "Loading data IndustryOR_fixed.json\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [02:34<00:00,  5.17it/s, est. speed input: 2199.02 toks/s, output: 4772.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset IndustryOR_fixed.json: 100\n",
      "Numbers of pass@8 cases in dataset IndustryOR_fixed.json: 39\n",
      "pass@8 accuracy for dataset IndustryOR_fixed.json: 39/100 = 0.39\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_193.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1544/1544 [09:32<00:00,  2.70it/s, est. speed input: 3230.20 toks/s, output: 5030.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_193.jsonl: 193\n",
      "Numbers of pass@8 cases in dataset OptMATH_Bench_193.jsonl: 68\n",
      "pass@8 accuracy for dataset OptMATH_Bench_193.jsonl: 68/193 = 0.35233160621761656\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptMATH_Bench_166.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1328/1328 [07:53<00:00,  2.81it/s, est. speed input: 3330.11 toks/s, output: 4967.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptMATH_Bench_166.jsonl: 166\n",
      "Numbers of pass@8 cases in dataset OptMATH_Bench_166.jsonl: 46\n",
      "pass@8 accuracy for dataset OptMATH_Bench_166.jsonl: 46/166 = 0.27710843373493976\n",
      "-------------------------------------------------------------------\n",
      "Loading data OptiBench.jsonl\n",
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4840/4840 [08:32<00:00,  9.44it/s, est. speed input: 3435.35 toks/s, output: 7643.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of test cases in dataset OptiBench.jsonl: 605\n",
      "Numbers of pass@8 cases in dataset OptiBench.jsonl: 379\n",
      "pass@8 accuracy for dataset OptiBench.jsonl: 379/605 = 0.6264462809917355\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# if you want to check pass@8 accuracy, please run this cell\n",
    "# Test the checkpoint\n",
    "datapath = 'test_data'\n",
    "testdataset = ['NL4OPT.jsonl', 'MAMO_EasyLP.json', 'MAMO_ComplexLP.json', 'IndustryOR_fixed.json', 'OptMATH_Bench_193.jsonl', 'OptMATH_Bench_166.jsonl','OptiBench.jsonl']\n",
    "for filepath in testdataset:\n",
    "    \n",
    "    # loading data\n",
    "    print('Loading data', filepath)\n",
    "    test_data = [i for i in load_jsonl(os.path.join(datapath, filepath)) for _ in range(8)]\n",
    "    print('Finish Loading')\n",
    "    \n",
    "    # generation \n",
    "    \n",
    "    prompt_list = []\n",
    "    for item in test_data:\n",
    "        prompt_list.append(mp_worker(item))\n",
    "    result_strs = generate_with_model(model, prompt_list, sampling_params)\n",
    "    snippet_package_cor = []\n",
    "    score = []\n",
    "    snippet_package_tmp=[]\n",
    "    # check the pass@8 accuracy\n",
    "    \n",
    "    result_chunks = [result_strs[i:i + 8] for i in range(0, len(result_strs), 8)]\n",
    "    test_data_chunks = [test_data[i:i + 8] for i in range(0, len(test_data), 8)]\n",
    "    for result_chunk, items in zip(result_chunks,test_data_chunks):\n",
    "        for chunk, item in zip(result_chunk, items):\n",
    "            snippet_package_tmp.append(check_result(chunk, item, solver_name))\n",
    "        if 1 in snippet_package_tmp:\n",
    "            snippet_package_cor.append(1)\n",
    "        else:\n",
    "            snippet_package_cor.append(0)\n",
    "        snippet_package_tmp.clear()\n",
    "    result = np.bincount(snippet_package_cor)\n",
    "    print(f'Numbers of test cases in dataset {filepath}: {sum(result)}')\n",
    "    print(f'Numbers of pass@8 cases in dataset {filepath}: {result[1]}')\n",
    "    print(f'pass@8 accuracy for dataset {filepath}: {result[1]}/{sum(result)} = {result[1] / sum(result)}')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SIRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
